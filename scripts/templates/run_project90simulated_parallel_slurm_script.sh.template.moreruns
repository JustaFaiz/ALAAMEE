#!/bin/bash


# use the 
# run_submit_project90simulated_parallel_slurm_script.sh
# job to submit this job and then process output with R scripts 
# on completion

#SBATCH --job-name="ALAAMEE_parallel_@JOBSUFFIX"
#SBATCH --time=0-28:00:00
#SBATCH --output=alaamee_project90sim-more-%j.out
#SBATCH --error=alaamee_project90sim-more-%j.err
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=50
#SBATCH --mem=16GB

uname -a
echo SLURM_CPUS_PER_TASK = $SLURM_CPUS_PER_TASK
echo -n "started at: "; date

module purge # otherwise module load python/3.9.0 sometimes fails
module load python/3.9.0

OLD_NUM_RUNS=@OLD_NUM_RUNS
NEW_NUM_RUNS=@NEW_NUM_RUNS
NEW_NUM_RUNS_MINUS_ONE=`expr $NEW_NUM_RUNS - 1`

export PYTHONPATH=${HOME}/ALAAMEE/python/:${PYTHONPATH}


# use default (no --jobs option) jobs in parallel (one per CPU default)
seq ${OLD_NUM_RUNS} ${NEW_NUM_RUNS_MINUS_ONE} | parallel  --progress --joblog parallel.log python3 ./runALAAMEEproject90simulatedParallel.py


times
echo -n "ended at: "; date
